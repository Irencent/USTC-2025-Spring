> **Problem 1.** 把最优传输问题转化为标准线性规划问题，并写出其对偶问题。

以下假设我们有两组离散分布：

* 源分布 $\{a_i\}_{i=1}^m$，满足 $a_i \ge 0$，$\sum_{i=1}^m a_i = 1$（或总和为某常数也可）。
* 目标分布 $\{b_j\}_{j=1}^n$，满足 $b_j \ge 0$，$\sum_{j=1}^n b_j = 1$。

记代价矩阵为 $C = (c_{ij})_{i=1,\dots,m}^{j=1,\dots,n}$，其中 $c_{ij}$ 表示将“质量”从源点 $i$ 运输到目标点 $j$ 的单位代价。记运输矩阵为 $P = (P_{ij})$，其中 $P_{ij}$ 是从 $i$ 到 $j$ 实际运输的质量。

最优传输问题的目标是：

$$
\min_{P} \; \sum_{i=1}^m \sum_{j=1}^n c_{ij}\,P_{ij}
$$

同时需满足以下约束：

1. 行和约束（源分布约束）：对每个 $i=1,\dots,m$，

   $$
   \sum_{j=1}^n P_{ij} \;=\; a_i.
   $$
2. 列和约束（目标分布约束）：对每个 $j=1,\dots,n$，

   $$
   \sum_{i=1}^m P_{ij} \;=\; b_j.
   $$
3. 非负性约束：

   $$
   P_{ij} \;\ge\; 0,\quad \forall\,i=1,\dots,m,\;j=1,\dots,n.
   $$

将上述内容写成标准线性规划（LP）形式，可记作：

$$
\begin{aligned}
&\text{(Primal)} \quad &&\min_{P_{ij}} && \sum_{i=1}^m \sum_{j=1}^n c_{ij}\,P_{ij} \\[6pt]
&\text{s.t.} && \sum_{j=1}^n P_{ij} = a_i, \quad && i = 1,\dots,m, \\[4pt]
&          && \sum_{i=1}^m P_{ij} = b_j, \quad && j = 1,\dots,n, \\[4pt]
&          && P_{ij} \ge 0, \quad && i=1,\dots,m,\;j=1,\dots,n.
\end{aligned}
$$

这里变量一共有 $m\times n$ 个：$\{P_{ij}\mid 1\le i\le m,\;1\le j\le n\}$。约束总数为 $m + n$ 个等式约束，加上非负性约束。


对上述线性规划进行对偶化。

   原目标：$\displaystyle \min_{P\ge 0} \sum_{i,j} c_{ij}P_{ij}$
   约束：

$$
   \sum_{j}P_{ij} = a_i,\quad i=1,\dots,m;\quad
   \sum_{i}P_{ij} = b_j,\quad j=1,\dots,n.
$$

   对偶变量：$\alpha_i$ 对应第 $i$ 个行和约束，$\beta_j$ 对应第 $j$ 个列和约束。则对应的对偶目标是

$$
   \max_{\alpha,\beta}\; \sum_{i=1}^m \alpha_i\,a_i \;+\; \sum_{j=1}^n \beta_j\,b_j
$$

   需要满足的对偶可行性条件（从拉格朗日函数对 $P_{ij}$ 的下界分析得出）是：

$$
   \alpha_i + \beta_j \;\le\; c_{ij},\quad \forall\,i=1,\dots,m,\;j=1,\dots,n.
$$

   同时对偶变量本身没有符号约束，可以取任意实数。于是：


$$
   \begin{aligned}
   &\text{(Dual)} \quad && \max_{\{\alpha_i\},\{\beta_j\}} && \sum_{i=1}^m \alpha_i\,a_i \;+\; \sum_{j=1}^n \beta_j\,b_j \\[6pt]
   &\text{s.t.} && \alpha_i + \beta_j \;\le\; c_{ij}, \quad && \forall\,i=1,\dots,m,\;j=1,\dots,n, \\[4pt]
   &          && \alpha_i,\;\beta_j \in \mathbb{R}, \quad && i=1,\dots,m,\;j=1,\dots,n.
   \end{aligned}
$$

---

> **Problem 2** 令 $X=\mathbb{R}^d$ ，并定义 $\mathcal{H}$ 为 $X$ 上的所有轴对齐矩形（axis－parallel boxes）构成的集合。具体来说：
> $$
> \mathcal{H}=\left\{h_{a, b} \mid a, b \in X\right\}
> $$
>
>
> 其中分类器 $h_{a, b}(x)$ 定义为：
>
> $$
> h_{a, b}(x)= \begin{cases}1 & \text { 如果 } a_i \leq x_i \leq b_i \text { 对所有 } i=1, \ldots, d \text { 成立, } \\ -1 & \text { 其他情况. }\end{cases}
> $$
>
>
> 比如，在二维平面上，假如 $\mathrm{a}=(-1,-1), \mathrm{b}=(1,1)$ ，那么矩形 $h_{a, b}$ 就表示 $((-1,-1),(-1,1),(1,1),(1$ ， －1）））四个点构成的矩形。
>
> 证明 $\mathcal{H}$ 的 VC dimension 是 2d
>


我们要证明 $\mathcal{H}$（在 $\mathbb{R}^d$ 上所有轴平行矩形分类器的集合）的 VC 维度等于 $2d$。通常分两步：

1. **下界（Lower bound）**：构造一个大小为 $2d$ 的点集，并证明 $\mathcal{H}$ 能够将这 $2d$ 个点“任意标记”（即任意二元标记模式都能被某个轴平行矩形分类器实现）。由此说明 $\mathrm{VCdim}(\mathcal{H})\ge2d$。

2. **上界（Upper bound）**：证明不存在能被 $\mathcal{H}$ 任意“打散”（shatter）的大小为 $2d+1$ 的点集。由此说明 $\mathrm{VCdim}(\mathcal{H})\le2d$。

合并两步可得 $\mathrm{VCdim}(\mathcal{H})=2d$。


我们在 $\mathbb{R}^d$ 中取以下 $2d$ 个典型点（以原点 $O=(0,0,\dots,0)$ 为中心）：

$$
\bigl\{
\,p^{(1)}_{-},\,p^{(1)}_{+},\;
p^{(2)}_{-},\,p^{(2)}_{+},\;\dots,\;
p^{(d)}_{-},\,p^{(d)}_{+}
\bigr\},
$$

其中对于第 $k$ 个维度（$k=1,2,\dots,d$），定义

$$
p^{(k)}_{-} \;=\; (0,\dots,0,\underbrace{-1}_{\text{第 }k\text{ 个坐标}},0,\dots,0), 
\quad
p^{(k)}_{+} \;=\; (0,\dots,0,\underbrace{+1}_{\text{第 }k\text{ 个坐标}},0,\dots,0).
$$

也就是说，在第 $k$ 个坐标轴的正负两个方向上，各取一个点，其它坐标都为 0。这样总共恰好有 $2d$ 个点。

记这个集合为

$$
S \;=\; \bigl\{\,p^{(k)}_{-},\,p^{(k)}_{+}\mid k=1,\dots,d\bigr\}, 
\quad |S|=2d.
$$


我们希望证明：**对于任意**将这 $2d$ 个点打二元标签（标记为 “+1” 或 “−1”）的方式，$\mathcal{H}$ 中都存在一个轴平行矩形 $h_{a,b}$ 使得它恰好把被标“+1”的那些点映为 +1，把被标“−1”的那些点映为 −1。

下面做具体构造。假设我们给这 $\{p^{(k)}_{-},\,p^{(k)}_{+}\}$ 做了某种二元标记。令

$$
I_{+} \;=\; \bigl\{\,k\in\{1,\dots,d\}\mid p^{(k)}_{+}\text{ 被标为 }+1\,\bigr\},
\quad
I_{-} \;=\; \bigl\{\,k\in\{1,\dots,d\}\mid p^{(k)}_{-}\text{ 被标为 }+1\,\bigr\}.
$$

即 $I_{+}$ 是那些在正方向上被标“+1”的维度索引，$I_{-}$ 是那些在负方向上被标“+1”的维度索引。其余没有在这两个集合内的点就被标为 “−1”。

为了用一个轴平行矩形把恰好这批被标“+1”的点挑出来，我们对每个坐标维度 $j=1,\dots,d$ 按如下方式选定矩形的下界 $a_j$ 和上界 $b_j$。

* 若 $j\in I_{+}$ 且 $j\in I_{-}$，说明在第 $j$ 个坐标轴的正负两端都需要标为 +1。但不可能“同时包含 $+1$ 和 $-1$ 方向” 的两个点，而又同时把它们包在一个开闭区间 $[a_j,b_j]$ 里，除非选择

  $$
  a_j \le -1,\quad b_j \ge +1  
  $$

  也就是说，第 $j$ 维的区间至少要覆盖 $-1$ 和 $+1$。这是允许的，只要满足这两个被标“+1”的点都能落入 $[a_j,b_j]$。

* 若 $j\in I_{+}$ 但 $j\notin I_{-}$，则说明“仅”需要把第 $j$ 维的 $p^{(j)}_{+}=(0,\dots,0, +1, 0,\dots,0)$ 标为 +1，而不包括 $p^{(j)}_{-}$。此时只要令

  $$
  a_j > -1,\quad b_j \ge +1 
  \quad\bigl(\text{例如 }a_j = 0,\;b_j=1\bigr)
  $$

  则 $[a_j,b_j]$ 会把 $+1$ 包含进来，但把 $-1$ 排除。

* 若 $j\in I_{-}$ 但 $j\notin I_{+}$，类似地选择

  $$
  a_j \le -1,\quad b_j < +1 
  \quad\bigl(\text{例如 }a_j=-1,\;b_j=0\bigr),
  $$

  此时把 $-1$ 包含进来，把 $+1$ 排除。

* 若 $j\notin I_{+}$ 且 $j\notin I_{-}$，即第 $j$ 维在两端都不需要标“+1”，此时我们只需令区间 $[a_j,b_j]$ 完全不覆盖 $\pm1$（例如 $a_j=1.5,\;b_j=2.0$）。这样既不会把第 $j$ 维的 $p^{(j)}_{\pm}$ 当成 +1，也不会影响其它坐标的判别。

将这 $d$ 个维度上分别挑定的区间 $[a_j,b_j]$ 组合起来，就得到如下的轴平行矩形：

$$
h_{a,b}(x) \;=\;
\begin{cases}
+1, 
&\text{若对 所有 }j=1,\dots,d,\;a_j \le x_j \le b_j;\\
-1,&\text{否则.}
\end{cases}
$$

显然，按上述选择，恰好 $\{\,x=p^{(k)}_{\pm}\mid (k,\pm)\text{ 在 }I_{+},I_{-}\text{ 中被标 }+1\}$ 的那些点会满足 $a_j \le (p^{(k)}_{\pm})_j \le b_j$（也就是落在矩形内部），被判为 +1；其余未标 +1 的点，因为在某个维度上跑出了 $[a_j,b_j]$，都会被判 −1。

这样，我们就能“任意给这 2d 个点打标签”，然后构造一个恰好实现该标签的矩形分类器。故

$$
\mathrm{VCdim}(\mathcal{H}) \;\ge\; 2d.
$$

要说明 $\mathrm{VCdim}(\mathcal{H})\le2d$，只需证明：**任意选取 $2d+1$ 个点的集合 $T\subset\mathbb{R}^d$，都存在一种二元标记，使得没有任何一个轴平行矩形能够恰好实现这份标记。** 换句话说，无论怎么标记，这  $\mathcal{H}$ 中的分类器都无法完美区分“+1”和“−1”。

下面给出两种常见的思路去证明上界：


令我们任取一个 $\;2d+1\;$ 大小的点集 $T=\{x^{(1)},x^{(2)},\dots,x^{(2d+1)}\}\subset\mathbb{R}^d$。



* 首先，由于有 $2d+1$ 个点，但每个点在 $d$ 个坐标维度上都有一个实数坐标，根据鸽巢原理，总有至少 **一个** 点 $x^*$ 在“所有 $d$ 个维度上都不是最大也不是最小”。——原因：若每个点在某个维度上都“要么最小，要么最大”，那最多也只是 “每个维度选 2 个极端位置，一共 $2d$ 个极端点”。但我们有 $2d+1$ 个点，必有一个点落在所有维度的“内部”而不是某个维度的极端。

* 取这样的点 $x^*$ 做为“唯一标记为 +1” 的点，其它的 $2d$ 个点都标为 −1。现在要证明：**没有** 任何一个轴平行矩形 $h_{a,b}$ 能做到恰好把 $x^*$ 判为 +1，把其余 $2d$ 个点判为 −1。

  令 $x^*=(x^*_1,x^*_2,\dots,x^*_d)$。因为 $x^*$ 在每个坐标维度都不是最小也不是最大，就可对每个 $j=1,\dots,d$ 选出

  $$
     \text{一个点 }\,L_j\in T 
     \quad\text{使得}\quad
     L_j\bigl[j\bigl] < x^*_j,
     \qquad
     \text{一个点 }\,U_j\in T 
     \quad\text{使得}\quad
     U_j\bigl[j\bigl] > x^*_j,
  $$

  这里 “$\,z[j]$” 表示点 $z$ 在第 $j$ 维的坐标。由于 $x^*$ 不是 第 $j$ 维的最小值，就必然存在一个 $L_j$ 使得 $L_j[j]<x^*_j$；同理，因 $x^*$ 不是第 $j$ 维的最大值，就有 $U_j[j]>x^*_j$。

  假设存在某个矩形 $h_{a,b}$（即对每个 $j$ 取 $[\,a_j,b_j]$）满足

  $$
    h_{a,b}(x^*) = +1, 
    \quad
    h_{a,b}(\,\text{任意其它 }x\in T\setminus\{x^*\}\,) = -1.
  $$

  由 $h_{a,b}(x^*)=+1$ 可知：

  $$
    \forall\,j=1,\dots,d,\quad a_j \;\le\; x^*_j \;\le\; b_j.
  $$

  但是考虑上面对于同一个 $j$ 选择的“在该维度上比 $x^*$ 要小”的点 $L_j$，因为 $L_j[j]<x^*_j$ 且 $a_j\le x^*_j$，就必有

  $$
    a_j \;\le\; L_j[j] \;<\; x^*_j \;\le\; b_j,
  $$

  因此 $L_j$ 也落在第 $j$ 维的区间 $[a_j,b_j]$ 之内。并且对其它的 $d-1$ 维度，是否满足也不用管，因为只要在 **每个维度** 上都要满足 $a_i \le x_i \le b_i$，那么如果在某个维度上就没有越界，整个点必然在矩形内。换句话说，若 $L_j$ 在第 $j$ 维就已经落在 $[a_j,b_j]$ 内，那么无论其它维度如何，这个点就满足“所有维度都落在矩形投影区间上”的条件，所以 $h_{a,b}(L_j)$ 必然被判 +1。这与我们希望 “$L_j$ 应该被判 −1”（因为它在 $T\setminus\{x^*\}$ 里）相矛盾。

  同理，“比 $x^*$ 在第 $j$ 维要大的点 $U_j$” 也必然满足 $x^*_j \le U_j[j]\le b_j$，一旦 $x^*_j\le b_j$，那么 $U_j[j]$ 也在 $[a_j,b_j]$ 内，从而 $U_j$ 也会被判 +1，这同样与我们要将所有其它点判为 −1 的目标冲突。

  由于对 “每个维度 $j$” 都会产生这样的矛盾，说明 **根本不可能用一个轴平行矩形同时把 $x^*$ 判 +1、又把所有它前后相邻的点都判 −1**。因此，这 $2d+1$ 个点集 $T$ 在我们所设计的标记方式下无法被 $\mathcal{H}$ 打散。故

  $$
  \mathrm{VCdim}(\mathcal{H}) \;\le\; 2d.
  $$

---

> **Problem 3.** 证明平面上的凸多边形的 VC dimension 是无穷

**1. 将点置于凸位置**

1. **选点位置**
   给定任意正整数 $m$，我们先在平面上任取一个圆（例如单位圆）：
   $$
     \mathcal{C} \;=\;\bigl\{\, (x,y)\in\mathbb{R}^2 : x^2 + y^2 = 1 \bigr\}.
   $$
   
   在该圆周上选取 $m$ 个两两不重合的点，记为
   
   $$
     P \;=\;\{\,p_1,p_2,\dots,p_m\}, 
     \quad p_k = \bigl(\cos\theta_k,\;\sin\theta_k\bigr), 
     \quad 0\le \theta_1 < \theta_2 < \cdots < \theta_m < 2\pi.
   $$
   
   由于这 $m$ 个点恰好全部落在同一个圆周上，并且按照极角顺时针依次排列，故它们处于“凸位置”（convex position）：即任意一个子集的凸包，其边界顶点只会来自被选点本身，而圆周上的其他点不会落在这个凸包内部。下面会反复利用这一点。
   
2. **凸位置的关键性质**

   * 若 $S\subseteq P$ 是任意非空子集，记 $\operatorname{Conv}(S)$ 为它在平面上的凸包（convex hull）。因为所有 $p_i$ 都在统一圆周上，且在凸包的构造过程中“不会把圆周上未选的点包含进来”，所以

     $$
       \operatorname{Conv}(S)\text{ 的边界顶点恰好就是 }S\text{ 本身（如果 }S\text{ 至少含 3 个点）；} 
     $$

     换言之，$\operatorname{Conv}(S)$ 不会包含任何 $P\setminus S$ 里的点。更具体地说：
   * 若 $|S|\ge3$，那么 $\operatorname{Conv}(S)$ 是一个由 $S$ 中点按极角顺序连线所成的凸多边形，其内部（含边界） **不会** 包含圆周上那些没有被选到 $S$ 的点。
   * 若 $|S|=1$ 或 $|S|=2$，那么它们的“凸包”要么是一个点（当 $|S|=1$），要么是一条线段（当 $|S|=2$）。这种情况我们稍后会借助“微小拐角”来扩充成真正的凸多边形，从而避免把其它圆周上的点误包含进来。

**2. 任意二元标记，都能找到一个凸多边形实现——即 $P$ 可被打散**

现在，我们针对这 $m$ 个点集 $P=\{p_1,\dots,p_m\}$，考虑它们上的 **任意** 二元标记（每个点标 +1 或 −1，总共有 $2^m$ 种可能）。固定一种标记方式，令

$$
  P_{+} \;=\;\{\,p_i\in P : \text{点 }p_i \text{ 被标为 }+1\}, 
  \qquad
  P_{-} \;=\;P \setminus P_{+}
$$

是被标为正例（+1）的子集，以及被标为反例（−1）的子集。我们要构造一个凸多边形 $H\subset\mathbb{R}^2$，满足

$$
  p\in P_{+} \;\Longrightarrow\; p\in H,
  \quad
  p\in P_{-} \;\Longrightarrow\; p\notin H.
$$

然后就令分类器 $h(x)=+1$ 当且仅当 $x\in H$；否则 $h(x)=-1$。那么这份分类器恰好实现了对 $P$ 上的这一次任意标记。下面按 $P_{+}$ 的大小分几种情况来构造：

---

#### 情形 1：$\boldsymbol{|P_{+}|\ge3}$

此时取

$$
  H \;:=\; \operatorname{Conv}\bigl(P_{+}\bigr),
$$

即令 $H$ 为正例点集 $P_{+}$ 的凸包。由于我们选的所有点都在同一个圆周上，所以

* $\operatorname{Conv}(P_{+})$ 本身就是一个“以 $P_{+}$ 中若干点为顶点的凸多边形（顶点数 $\ge3$）”，
* 而且由于 $P_{+}\subset P\subset\mathcal{C}$，其凸包的判断是：连线这些极角最外层的顶点，最终得到的多边形 **不会** 将任何 $\mathcal{C}$ 上、却不在 $P_{+}$ 中的点包含在内部（任何不在 $P_{+}$ 中的 $p_i$ 全部落在圆周上，并且根据圆弧与对应弦的几何关系，都在凸包边沿之外或在外部）。
  因此：

1. 对于每个 $p_i\in P_{+}$，显然 $p_i\in \operatorname{Conv}(P_{+})$，所以会被分类成 +1。
2. 对于每个 $p_j\in P_{-}$，它既不在 $P_{+}$ 中，也不可能落在 $\operatorname{Conv}(P_{+})$ 的内部（因为所有的边都沿着子集点和圆心向外，任何圆周上的其他点都在多边形弧面之外或仅与边共线于边界，但我们可以稍稍将多边形做开放一点以保证排除）。因此它被判为 −1。

这样，一举构造了一个凸多边形恰好将“正例集”与“反例集”完全区分。故当 $|P_{+}|\ge3$ 时，**必然**能找到一个凸多边形把标记实现。

---

#### 情形 2：$\boldsymbol{|P_{+}| = 2}$

假设正例点恰好是 $\{\,p_i,p_j\}$ 两个。由于这两个点都在圆 $\mathcal{C}$ 上，且它们之间沿圆周的弧不是整个圆周，连成线段会把弧上的其它点分到线段的外侧（弦在圆心一侧将圆弧其他点都排除在外）。但是“仅仅一条线段”并不是一个“凸多边形”——我们需要一个真正的多边形来做分类区域。做法很简单：在这两个正例点的连线（弦）附近构造一个 “非常细的三角形” 或“四边形”，使其既包含这两点、又不包含圆周上其余任何一个点。具体而言：

1. 令这两个正例点按极角顺序为 $p_i=(\cos\theta_i,\sin\theta_i)$ 和 $p_j=(\cos\theta_j,\sin\theta_j)$，且不失一般性，假设 $\theta_i<\theta_j$。
2. 考虑以下三角形顶点：

   $$
     t_1 \;=\; (\cos\theta_i,\sin\theta_i), 
     \quad
     t_2 \;=\; (\cos\theta_j,\sin\theta_j),
     \quad
     t_3 \;=\; \bigl( (1-\varepsilon)\cos\bigl(\tfrac{\theta_i+\theta_j}{2}\bigr),\; (1-\varepsilon)\sin\bigl(\tfrac{\theta_i+\theta_j}{2}\bigr)\bigr),
   $$

   这里 $\varepsilon>0$ 是一个极小的正数，足够小以保证：

   * $t_3$ 落在两点 $p_i,p_j$ 所跨弦（在圆心与弦连线向内的那一小段）附近、但离弦稍微靠里一点，
   * 并且 **不** 落在“其他任何圆周点” 的正方形非常小邻域内。
     于是三角形 $\triangle(t_1,t_2,t_3)$ 就是一个凸多边形（3 边）。由于 $\varepsilon$ 取得足够小：
   * 它必定包含 $p_i$ 和 $p_j$ 这两个顶点本身；
   * 而对于任何 $p_k\in P\setminus\{p_i,p_j\}$，要么它的极角 $\theta_k$ 不在 $[\theta_i,\theta_j]$ 对应的“短弧”上，因而连到弦的直线会把它判到三角形外（显然不被包含）；要么它在“短弧”中，但它恰好在圆周上，而三角形第三点 $t_3$ 选在距离圆心更近的位置，使得这条点 $p_k$ 通常会落在三角形“边延长线”之外——换言之，我们刻意让三角形的底沿着弦，顶点退到内部，从而将弦以外的所有圆周点都挤到三角形外部。
     这样，这个三角形就恰好把“正例” $\{p_i,p_j\}$ 包含进来，而把其余 $m-2$ 个点全部排除在外。

故当 $|P_{+}|=2$ 时，也可以构造一个凸三角形（多边形）将二元标记实现。

---

#### 情形 3：$\boldsymbol{|P_{+}| = 1}$

若正例集只有一个点 $\{p_i\}$，我们只需围绕 $p_i$ 做一个“非常小的三角形”或“四边形”，把它包进去，同时保证其它 $m-1$ 个圆周上的点都不在这个三角形（或四边形）内部即可。做法与上一情形类似：取该点 $p_i=(\cos\theta_i,\sin\theta_i)$，在它的极角方向上稍微“往圆心靠里挖一点”出第三个顶点，再加上两个几乎与它重合但稍稍偏离的点，就能构造一个包含它、却不包含圆周上其他任何一点的凸三（或四）边形。细节如上，不再赘述。

---

#### 情形 4：$\boldsymbol{|P_{+}| = 0}$

即没有正例，所有 $P$ 中的点都标为 −1。此时只需让分类器对应一个“不包含任何 $P$ 上点”的凸多边形——最简单的做法是令 $H$ 为“平面上某个与圆周 $\mathcal{C}$ 相距大于 1 的微小凸多边形”（例如选取一个完全在原点外部、且与单位圆不相交的很小三角形或小正方形）。这样所有 $p_i$ 都在那个三角形外面，就被判为 −1，正好实现了“全标负”的情形。

---

以上把所有可能的 $\;|P_{+}|=0,1,2,\dots,m\;$ 情形都考虑到，证明了：对于我们在圆周上取的任意 $m$ 个点 $P$，**无论怎么给它们做正/负二元标记**，都能在“平面上的凸多边形”这一分类族中找到一个多边形，把正例恰好划分为内部，把反例恰好划分为外部。换句话说，这个点集 $P$ 被“凸多边形”族打散（shatter）。

因为 $m$ 可以任意大，于是得出：

$$
\mathrm{VCdim}\bigl(\text{“平面上的凸多边形”}\bigr)\;=\;\infty.
$$

---

> **Problem 4.** 设 $a_1, a_2, \ldots, a_n$ 为一个符号流，其中每个符号是集合 $\{1, \ldots, m\}$ 中的一个整数。
> 1．均匀随机选择：设计一个算法，从流中均匀随机选择一个符号。你的算法需要多少内存？
> 2．加权随机选择：设计一个算法，以与 $a_i^2$ 成正比的概率选择一个符号。

**1. 均匀随机选择（Reservoir Sampling）**

我们不知道流的总长度，流元素依次到达，记第 $i$ 个到达的元素为 $a\_i\in{1,\dots,m}$。我们要从整个流中以相同概率选出一个元素（即第 $i$ 个元素被选中的概率始终为 $1/N$，其中 $N$ 是流的总长度），但只能一边扫描一边做计算，并且只能用很少的内存。

**算法（“容量为 1 的水塘抽样”）**

  1. 令 `sample` 为当前保留的候选元素，令 `count`=0。
  2. 对于流中每一个新到来的元素 $a_i$（第 $i$ 个元素），执行：

     1. 先将 `count` 加 1。
     2. 以概率 $\frac{1}{\texttt{count}}$ 将 `sample` 更新为 $a\_i$；否则保持原来的 `sample` 不变。
  3. 流读完后，输出 `sample` 作为结果。

**正确性简要说明**

  * 当只看到第 1 个元素时，$\texttt{count}=1$，算法以概率 1 选 $a_1$；显然此时只有 $a_1$ 可选。
  * 当看到第 $i$ 个元素 $a_i$ 时，$\texttt{count}=i$，算法以概率 $1/i$ 让 $\texttt{sample}=a_i$。令 $S_k$ 表示“在第 $k$ 步之后保留的样本”，我们可以归纳地证明：到第 $i$ 步时，每个 ${a_1,\dots,a\_i}$ 中的元素被保留的概率都是 $1/i$。

    * 具体地，假设在第 $i-1$ 步时，对任意 $j<i$，$\Pr(S_{i-1}=a_j)=1/(i-1)$。
    * 第 $i$ 步来新的 $a_i$，我们以 $1/i$ 的概率替换为 $a_i$，此时 $\Pr(S_i=a_i)=1/i$。
    * 对于任意 $j<i$，要想在第 $i$ 步后仍保留 $a_j$，条件是“第 $i$ 步没有用 $a_i$ 替换掉旧样本”且之前 $a_j$ 已经是样本，因此

      $$
        \Pr\bigl(S_i = a_j\bigr)
        \;=\;
        \Pr\bigl(S_{i-1}=a_j\bigr)\times\Pr(\text{“第 $i$ 步不替换”})
        \;=\;
        \frac{1}{i-1}\times \Bigl(1-\frac{1}{i}\Bigr)
        \;=\;
        \frac{1}{i}\,.
      $$
    * 由此归纳，第 $i$ 步结束时，每个 $a_1,dots,a_i$ 的保留概率均为 $1/i$。当流结束共有 $N$ 个元素到达时，每个元素被选中的概率恰好都是 $1/N$。

* **所需内存**

  * 仅需维护两个量：一个变量 $\texttt{count}$（当前已读元素个数），和一个变量 $\texttt{sample}$（当前保留的候选元素）。
  * 因此内存开销是 $\mathbf{O(1)}$（常量级），只要能存一个整数 $a_i$ 和一个计数器就足够。

---

**2. 加权随机选择（按 $a_i^2$ 加权抽样）**

* **问题描述**：给定同样的流 $a_1,a_2,dots,a_n$，但这次希望第 $i$ 个元素 $a_i$ 被选中的概率 $\displaystyle\propto a_i^2$。换句话说，如果整个流有 $N$ 个元素，则要使

  $$
    \Pr\bigl(\text{最终选出 }a_i\bigr)
    \;=\;
    \frac{\,a_i^2\,}{\sum_{j=1}^N a_j^2}\,.
  $$

  依然要求一次遍历，且尽量只用 \$O(1)\$ 或 \$O(\log n + \log m)$ 空间。

* **算法（Weighted Reservoir Sampling，规模 1）**

  1. 令 $\texttt{sample}$ = “空”，令 $\texttt{W} = 0$，表示当前已读取到元素的**累计权重和**。
  2. 当第 $i$ 个元素 $a_i$ 到达时，先计算它的权重 $w_i := a_i^2$，然后更新累计权重：

     $$
       \texttt{W} \;:=\; \texttt{W} + w_i.
     $$
  3. 再以概率

     $$
       \frac{w_i}{\texttt{W}}
     $$

     用 $a_i$ 替换当前的 $\texttt{sample}$；否则，不变。
     也就是说，生成一个 $u\sim \mathrm{Uniform}[0,1]$，如果 $u < \frac{w_i}{\texttt{W}}$，就让 $\texttt{sample}$ = $a_i$，否则保持原来的$\texttt{sample}$。
  4. 流结束时，输出 $\texttt{sample}$ 作为加权抽样结果。

**正确性直观说明**
  下面简单说明为何“第 $i$ 步以 $w_i/(\sum_{j=1}^i w_j)$ 替换”能最终保证“$a_i$ 的被选概率与 $w_i$ 成正比”。

  令 $W^{(i)}=\sum_{j=1}^i w_j$ 是前 $i$ 步的权重和，记 $S_i$ 为“第 $i$ 步结束时的样本”。我们要证明，当流共有 $N$ 个元素时，对任意 $i$ 有

$$
    \Pr\bigl(S_N = a_i\bigr)
    \;=\;
    \frac{w_i}{\sum_{j=1}^N w_j}
    \;=\;
    \frac{a_i^2}{\sum_{j=1}^N a_j^2}.
$$

**第 1 步**：$W^{(1)}=w_1$，算法以概率 $w_1/w_1=1$ 选 $a_1$ 作为样本，显然 $\Pr(S_1=a_1)=1=\frac{w_1}{w_1}$。
  * **假设** 对某个 $i-1$，在读完第 $i-1$ 步时，对于所有 $j\le i-1$，都成立

    $$
      \Pr\bigl(S_{i-1}=a_j\bigr)
      \;=\;
      \frac{\,w_j\,}{\sum_{k=1}^{\,i-1\,}w_k}\;=\;\frac{w_j}{W^{(i-1)}}.
    $$

**看第 $i$ 步**（加入 $a_i$）：此时新的累计权重 $W^{(i)}=W^{(i-1)}+w_i$。

1. $\Pr\bigl(S_i = a_i\bigr)$ 要分两种情况：在第 $i$ 步“被选”且前面无须保留任何旧的。显然第 $i$ 步以概率 $w_i/W^{(i)}$ 把 $a_i$ 选为新样本，所以

   $$
     \Pr\bigl(S_i = a_i\bigr)
     \;=\;
     \frac{w_i}{W^{(i)}}\,.
   $$
2. 对于任意 $j<i$，要让 $a_j$ 最终出现在 $S_i$ 中，需要满足两件事：

   * 在第 $i-1$ 步时，样本就是 $a_j$；且
   * 在第 $i$ 步没有被 $a_i$ 替换。
     于是

   $$
     \Pr\bigl(S_i = a_j\bigr)
     \;=\;
     \Pr\bigl(S_{i-1}=a_j\bigr)\;\times\;\Pr\bigl(\text{第 }i\text{ 步“不替换”}\bigr).
   $$

根据归纳假设，$\Pr(S_{i-1}=a_j)=w_j/W^{(i-1)}$；而“第 $i$ 步不替换”的概率是

$$
    1 - \frac{w_i}{\,W^{(i)}\,}
    \;=\;
    \frac{\,W^{(i)} - w_i\,}{\,W^{(i)}\,}
    \;=\;
    \frac{W^{(i-1)}}{\,W^{(i)}\,}.
$$

于是

$$
    \Pr\bigl(S_i = a_j\bigr)
    \;=\;
    \frac{\,w_j\,}{W^{(i-1)}} \;\times\; \frac{\,W^{(i-1)}\,}{\,W^{(i)}\,}
    \;=\;
    \frac{\,w_j\,}{W^{(i)}}\,.
$$

由此归纳完成：对任意 $i\le N$，第 $i$ 个元素被保留到最终结果的概率都是

$$
\Pr\bigl(S_N = a_i\bigr)
\;=\;
\frac{w_i}{\sum_{k=1}^N w_k}
\;=\;
\frac{a_i^2}{\sum_{k=1}^N a_k^2},
$$

刚好满足题目要求的“与 $a_i^2$ 成正比”。

**所需内存**

除了用于存储当前保留的 $\texttt{sample}$ 和一个累加器 $\texttt{W}$ 外，不需要保存其他历史信息。
$\texttt{W}$ 的数值范围是 $\sum a_j^2$，如果 $n$ 较大、每个 $a_j\le m$，那么 $\texttt{W}$ 不超过 $n,m^2$，只要 $O(\log(nm))$ 位就够表示。
因此总体上依旧是 **$O(1)$（常量）级别** 的额外内存开销。

---

> **Problem 5.** 设 k－median 的优化目标为 $f(P, C)$ ，相应的距离度量为 $g \circ A=\left\{a_1, \ldots, a_m\right\}$ 是一个 $\alpha, \beta$－近似解，即 $f(P, A) \leq \alpha f\left(P, C_{o p t}\right)$ ，且 $m \leq \beta k$ 。以每个 $\alpha_j$ 为圆心，使用以 $2^t R$ 为半径的若干同心圆来划分空间，其中 $R=\frac{1}{\alpha} f(P, A), t=0,1, \ldots, \phi, \phi=\log \alpha n$ 。对于每个 $j, t$ ，设同心圆环内部的点集为 $N_j^t$ ，我们在每个 $N_j^t$ 中都取 $x=\Theta\left(\frac{1}{\epsilon_0^2} \log \frac{1}{\lambda}\right)$ 个点组成 coreset。那么对一个固定的 k－median 解 $C$ ，依一定的概率，我们有 $\left|\sum_{p \in S_j^t} \frac{\left|N_j^t\right|}{x} g(p, C)-\sum_{p \in N_j^t} g(p, C)\right| \leq \epsilon_0 2^{t+1} R\left|N_j^t\right| \quad$（Hoeffding Bound）．
>
> 试证明：
>
> $$
> \left|\sum_{j=1}^m \sum_{t=0}^\phi \sum_{p \in S_j^t} \frac{\left|N_j^t\right|}{x} g(p, C)-\sum_{j=1}^m \sum_{t=0}^\phi \sum_{p \in N_j^t} g(p, C)\right| \leq \Theta\left(\epsilon_0\right) n f(P, A)
> $$
>
> （提示：课上证明了若只考虑 $t \geq 1$ 层的同心圆环内的点集 $N_j^t$ ，不等式右侧 $\leq 4 \epsilon_0 n f(P, A)=\Theta\left(\epsilon_0\right) n f(P, A)$ ，这个结论成立。现在需证明在考虑 $t=0$ 层的点集 $N_j^0$ 的情况下，该结论依然成立）
>


如题意和课堂所证明，对于每一个近似中心 α\_j 以及任意 t ≥ 1，一定概率下有

$$
\Biggl|\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)\;-\;\sum_{p\in N_j^t} g(p,C)\Biggr|
\;\le\;
\epsilon_0 \,2^{\,t+1}\,R\,|N_j^t|\quad\text{（对每个 }j,t\text{ 都成立）}.
$$

将所有 $j=1,\dots,m$ 以及所有 $t=1,2,\dots,\phi$ 的这些不等式累加，就得到

$$
\Biggl|\;\sum_{j=1}^m \sum_{t=1}^{\phi}
   \Bigl(\sum_{p\in S_j^t}\tfrac{|N_j^t|}{x}\,g(p,C)\;-\;\sum_{p\in N_j^t}g(p,C)\Bigr)\Biggr|
\;\le\;
\sum_{j=1}^m\sum_{t=1}^{\phi}
   \epsilon_0\,2^{\,t+1}\,R\,|N_j^t|.
$$

由于课堂中已经证明（或可见的标准推导）原式右端

$$
\sum_{j=1}^m\sum_{t=1}^{\phi} 2^{\,t+1}\,R\,|N_j^t|
\;\le\;
4\,n\,f(P,A),
$$

从而得出

$$
\Biggl|\sum_{j=1}^m\sum_{t=1}^{\phi}\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)\;-\;
           \sum_{j=1}^m\sum_{t=1}^{\phi}\sum_{p\in N_j^t} g(p,C)\Biggr|
\;\le\;
\epsilon_0\,\Bigl(4\,n\,f(P,A)\Bigr)
\;=\;4\,\epsilon_0\,n\,f(P,A).
\tag{1}
$$


接下来只需把 t=0 的那一层再加进来，检验它对总误差的贡献不会超过一个 $O(\epsilon_0)\,n\,f(P,A)$ 的量级。


对每个近似中心 $\alpha_j$（共有 $m\le\beta\,k$ 个），我们以半径 $2^t R$ 划分同心圆：

$$
    t=0\text{ 层：半径 }2^0 R = R;
    \quad
    t=1\text{ 层：半径 }2^1 R = 2R;
    \quad\ldots
$$
因此 “t=0 的环” $N_j^0$ 就是

$$
    N_j^0
    \;=\;
    \bigl\{\,p\in P:
    \;g\bigl(p,\alpha_j\bigr)\le R,\; \text{且 }p\text{ 属于第 }j\text{ 个近似中心的簇}\bigr\}.
$$

在该环内我们抽取定量样本 $S_j^0$，其与整层的加权误差（Hoeffding 估计）满足

$$
    \Bigl|\sum_{p\in S_j^0} \tfrac{|N_j^0|}{x}\,g(p,C)
        \;-\; \sum_{p\in N_j^0}g(p,C)\Bigr|
    \;\le\; \epsilon_0 \,2^{\,0+1}\,R\,|N_j^0|
    \;=\; 2\,\epsilon_0\,R\,|N_j^0|.
$$

于是对所有 $j=1,\dots,m$ 求和可得 t=0 层的总体误差：

$$
\Bigl|\;\sum_{j=1}^m \Bigl(
    \sum_{p\in S_j^0}\tfrac{|N_j^0|}{x}\,g(p,C)
    \;-\;\sum_{p\in N_j^0} g(p,C)\Bigr)\Bigr|
\;\le\;
\sum_{j=1}^m \,2\,\epsilon_0\,R\,|N_j^0|
\;=\;
2\,\epsilon_0\,R\;\sum_{j=1}^m |N_j^0|.
\tag{2}
$$

**继续估计 $\sum_{j=1}^m |N_j^0|$：**
记

$$
M_0 \;=\;\sum_{j=1}^m |N_j^0|
\;=\;\Bigl|\bigcup_{j=1}^m N_j^0\Bigr|
\quad(\text{由于同一个点只会属于自己所属近似中心的某个 }N_j^0\text{，因此 }\sum|N_j^0|\le n).
$$

显然

$$
M_0 \;\le\; n,
$$

因为 $N_j^0\subseteq P$，且不同的 $j$ 不会重复计数同一个流中点。


题中给出

$$
R \;=\; \frac{1}{\alpha}\,f\bigl(P,A\bigr),
$$

其中 $f(P,A)$ 是选定近似中心集合 $A$ 时的 k–median 目标值。于是

$$
2\,\epsilon_0\,R\,M_0
\;\le\;
2\,\epsilon_0\,\Bigl(\tfrac{1}{\alpha}\,f(P,A)\Bigr)\,M_0
\;\le\;
2\,\epsilon_0\,\Bigl(\tfrac{1}{\alpha}\,f(P,A)\Bigr)\,n
\;=\;
\frac{2\,\epsilon_0}{\alpha}\;n\;f(P,A).
$$

记住 $\alpha$ 是近似因子（常数），因此

$$
2\,\epsilon_0\,R\,M_0 \;=\; O\bigl(\epsilon_0\,n\,f(P,A)\bigr).
$$

换句话说，t=0 层的总体加权误差满足

$$
\Bigl|\;\sum_{j=1}^m \sum_{p\in S_j^0} \tfrac{|N_j^0|}{x}\,g(p,C)
    \;-\;\sum_{j=1}^m \sum_{p\in N_j^0}g(p,C)\Bigr|
\;\le\;
\frac{2\,\epsilon_0}{\alpha}\;n\,f(P,A)
\;=\;
\Theta\bigl(\epsilon_0\,n\,f(P,A)\bigr).
\tag{3}
$$


现在，把（1）式中的 t ≥ 1 部分与（3）式中的 t = 0 部分合并，得到整个所有层（t=0,1,…,φ）的误差上界：

$$
\begin{aligned}
&\Bigl|\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)
\;-\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in N_j^t}g(p,C)\Bigr| \\[6pt]
\le\;&
\Bigl|\;\sum_{j=1}^m\sum_{t=1}^{\phi}\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)
\;-\;\sum_{j=1}^m\sum_{t=1}^{\phi}\sum_{p\in N_j^t}g(p,C)\Bigr|
\\[4pt]
&\quad\;+\;
\Bigl|\;\sum_{j=1}^m\sum_{p\in S_j^0}\frac{|N_j^0|}{x}\,g(p,C)
\;-\;\sum_{j=1}^m\sum_{p\in N_j^0}g(p,C)\Bigr|.
\end{aligned}
$$

代入（1）与（3）的上界，得

$$
\Bigl|\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)
\;-\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in N_j^t}g(p,C)\Bigr|
\;\le\;
4\,\epsilon_0\,n\,f(P,A)
\;+\;
\frac{2\,\epsilon_0}{\alpha}\,n\,f(P,A).
$$

因为 $\alpha$ 是常数（近似因子），上述右侧可以写成

$$
\Bigl(4 \;+\;\tfrac{2}{\alpha}\Bigr)\,\epsilon_0\;n\;f(P,A)
\;=\;\Theta\bigl(\epsilon_0\,n\,f(P,A)\bigr).
$$

这就是所要证明的：

$$
\boxed{
\Bigl|\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in S_j^t}\frac{|N_j^t|}{x}\,g(p,C)
\;-\;\sum_{j=1}^m\sum_{t=0}^{\phi}\sum_{p\in N_j^t}g(p,C)\Bigr|
\;\le\;
\Theta\bigl(\epsilon_0\bigr)\;n\;f(P,A).
}
$$

---

> **Problem 6.** 证明（关于欧氏 $k$－means 问题的）coresets 满足下面的可组合性质（composability）：
> 令 $A_1, A_2 \subseteq \mathbb{R}^d$ 是两个互不相交的集合。假设集合 $S_1$ 及权重函数 $w: S_1 \rightarrow \mathbb{R}$ 和集合 $S_2$ 及权重函数 $w: S_2 \rightarrow \mathbb{R}$ 分别是 $A_1$ 和 $A_2$ 的 $(k, \varepsilon)$－coresets。那么 $S_1 \cup S_2$ 及函数 $w_1+w_2: S_1 \cup S_2 \rightarrow \mathbb{R}$ 是 $A_1 \cup A_2$ 的 （ $k, \varepsilon$ ）－coreset。
>
> 注：这里 $w_1+w_2$ 的定义如下：
>
> $$
> \left(w_1+w_2\right)(x)= \begin{cases}w_1(x), & \text { 如果 } x \in S_1 \backslash S_2, \\ w_2(x), & \text { 如果 } x \in S_2 \backslash S_1, \\ w_1(x)+w_2(x), & \text { 如果 } x \in S_2 \cap S_1 .\end{cases}
> $$
>




因为 $A_1$ 与 $A_2$ 互不相交，对于同一个中心集合 $C$，整个集合 $A=A_1\cup A_2$ 到 $C$ 的平方距离和是两个部分之和：

$$
  \mathrm{cost}_A(C)
  \;=\;
  \sum_{p\in A} \min_{c\in C}\|\,p-c\|^2
  \;=\;
  \sum_{p\in A_1} \min_{c\in C}\|\,p-c\|^2
  \;+\;
  \sum_{p\in A_2} \min_{c\in C}\|\,p-c\|^2
  \;=\;
  \mathrm{cost}_{A_1}(C)\;+\;\mathrm{cost}_{A_2}(C).
$$

同理，因为我们在合并后的点集 $S=S_1\cup S_2$ 上，将权重定义为

$$
  w(x) \;=\;
  \begin{cases}
    w_1(x), & x\in S_1\setminus S_2,\\
    w_2(x), & x\in S_2\setminus S_1,\\
    w_1(x)+w_2(x), & x\in S_1\cap S_2,
  \end{cases}
$$

那么合并后加权代价 $\mathrm{cost}_S(C)$ 也可以拆成两部分：

$$
  \mathrm{cost}_S(C)
  \;=\;
  \sum_{q\in S} w(q)\,\min_{c\in C}\|\,q-c\|^2
  \;=\;
  \underbrace{\sum_{q\in S_1} w_1(q)\,\min_{c\in C}\|\,q-c\|^2}_{\displaystyle\mathrm{cost}_{S_1}(C)}
  \;+\;
  \underbrace{\sum_{q\in S_2} w_2(q)\,\min_{c\in C}\|\,q-c\|^2}_{\displaystyle\mathrm{cost}_{S_2}(C)}.
$$

（这里主要用到：若 $q$ 同时属于 $S_1$ 和 $S_2$，那么它对合并后加权代价的贡献是 $\bigl(w_1(q)+w_2(q)\bigr)\min_{c\in C}\|q-c\|^2$，而这恰好等于将它在 $\mathrm{cost}_{S_1}$ 与 $\mathrm{cost}_{S_2}$ 中的两次计数相加。）


根据题意，$(S_1,w_1)$ 是 $A_1$ 的 $(k,\varepsilon)$–coreset，所以对任意的 $k$ 个中心 $C$ 都有

$$
  (1 - \varepsilon)\,\mathrm{cost}_{A_1}(C)
  \;\le\;
  \mathrm{cost}_{S_1}(C)
  \;\le\;
  (1 + \varepsilon)\,\mathrm{cost}_{A_1}(C).
$$

同样，$(S_2,w_2)$ 是 $A_2$ 的 $(k,\varepsilon)$–coreset，所以

$$
  (1 - \varepsilon)\,\mathrm{cost}_{A_2}(C)
  \;\le\;
  \mathrm{cost}_{S_2}(C)
  \;\le\;
  (1 + \varepsilon)\,\mathrm{cost}_{A_2}(C).
$$

我们把这两条不等式相加，得到对同一个 $C$：

$$
  \bigl(1 - \varepsilon\bigr)\,\mathrm{cost}_{A_1}(C)
  \;+\;
  \bigl(1 - \varepsilon\bigr)\,\mathrm{cost}_{A_2}(C)
  \;\le\;
  \mathrm{cost}_{S_1}(C)\;+\;\mathrm{cost}_{S_2}(C)
  \;\le\;
  \bigl(1 + \varepsilon\bigr)\,\mathrm{cost}_{A_1}(C)
  \;+\;
  \bigl(1 + \varepsilon\bigr)\,\mathrm{cost}_{A_2}(C).
$$

由于 $\mathrm{cost}_S(C)=\mathrm{cost}_{S_1}(C)+\mathrm{cost}_{S_2}(C)$ 且
$\mathrm{cost}_A(C)=\mathrm{cost}_{A_1}(C)+\mathrm{cost}_{A_2}(C)$，上式可以写为

$$
  (1 - \varepsilon)\,\bigl[\mathrm{cost}_{A_1}(C)+\mathrm{cost}_{A_2}(C)\bigr]
  \;\le\;
  \mathrm{cost}_S(C)
  \;\le\;
  (1 + \varepsilon)\,\bigl[\mathrm{cost}_{A_1}(C)+\mathrm{cost}_{A_2}(C)\bigr].
$$

也就是

$$
  (1 - \varepsilon)\,\mathrm{cost}_A(C)
  \;\le\;
  \mathrm{cost}_S(C)
  \;\le\;
  (1 + \varepsilon)\,\mathrm{cost}_A(C).
$$

这正是说 $(S,w)$ 满足对任意 $k$ 个中心集合 $C$ 来说，误差在 $\varepsilon$ 以内，恰好满足 $(k,\varepsilon)$–coreset 的定义。

---

> **Problem 7.** 假设 $\alpha \in(0,1]$ 。假如我们将（基本的）Morris 算法修改如下：
> （a）初始化 $X \leftarrow 0$ 。
> （b）对于每次更新，以 $\frac{1}{(1+\alpha)^X}$ 的概率使 $X$ 增加 1 。
> （c）对于查询，输出 $\hat{n}=\frac{(1+\alpha)^{X-1}}{\alpha}$ 。
> 记 $X_n$ 为上述算法中 $n$ 次更新以后的 $X$ 。令 $\hat{n}=\frac{(1+\alpha)^{X_{n-1}}}{\alpha}$ 。
>
> - 计算 $\mathbb{E}[\hat{n}]$ 和 $\operatorname{Var}[\hat{n}]$ 。
> - 假设 $\epsilon, \delta \in(0,1)$ 。基于上述算法，给出一个新算法，使得新算法以至少 $1-\delta$ 的概率输出一个估计值 $\hat{n}$ ，满足 $|\hat{n}-n| \leq \epsilon n$ 。说明你的算法的正确性与（最坏）空间复杂度（即算法使用的比特数）。你的算法只需要满足以至少 $1-\delta^{\prime}$ 的概率，其最坏空间复杂度为关于 $\frac{1}{\delta}, \frac{1}{\delta^{\prime}}, \frac{1}{\epsilon}$ 和 $\log \log n$ 的多项式（即 $\left.\operatorname{poly}\left(\frac{1}{\delta}, \frac{1}{\delta^{\prime}}, \frac{1}{\epsilon}, \log \log n\right)\right)$ 。（ $\alpha$ 是常数）
>
> 提示：这道题的解答参考课上讲的 Morris 算法和 Morris＋算法的证明
>

**1. 计算 $\mathbb{E}[\hat n]$ 和 $\mathrm{Var}[\hat n]$**

为了计算 $\mathbb{E}[\hat n]$ 与 $\mathrm{Var}[\hat n]$，我们先定义

$$
  W_t \;=\; (\,1+\alpha\,)^{\,X_t}\qquad(t=0,1,2,\dots).
$$

那么

$$
  \hat n \;=\; \frac{\,W_{\,n-1}\,}{\alpha} \,.
$$

于是先分别对 $W_t$ 求一二阶矩，再根据线性变换得到 $\hat n$ 的一二阶矩，最后由此求出 $\mathbb{E}[\hat n]$ 与 $\mathrm{Var}[\hat n]$。


假设在第 $t-1$ 步时，$X_{t-1}=x$，那么当进行第 $t$ 次更新时：

* 有概率 $\displaystyle \frac{1}{(1+\alpha)^x} = \frac{1}{W_{\,t-1}}$ 令 $X_t = x+1$，此时 $W_t = (1+\alpha)^{\,x+1} = (1+\alpha)\,W_{\,t-1}.$
* 有概率 $1 - \frac{1}{W_{\,t-1}}$ 令 $X_t = x$，此时 $W_t = W_{\,t-1}.$

故条件在 $W_{\,t-1}=w$ 时，

$$
\begin{align*}
\mathbb{E}\bigl[\,W\_t \mid W_{\,t-1} = w\bigr]
&= \biggl(1 - \frac{1}{w}\biggr)\times w \;+\;\frac{1}{w}\times\bigl((1+\alpha),w\bigr)\\
&= w \;-\; 1 \;+\; (1+\alpha) \;=\; w \;+\; \alpha.
\end{align*}
$$

也就是说，不论此时 $W_{\,t-1}$ 具体取何值，做一次更新后

$$
  \mathbb{E}\bigl[\,W_t \mid W_{\,t-1}\bigr]
  \;=\; W_{\,t-1} \;+\;\alpha.
$$

由于 $W_0=(1+\alpha)^{X_0}=(1+\alpha)^0 = 1$，递归地做全期望可得

$$
  \mathbb{E}\bigl[\,W_t\bigr]
  \;=\; \mathbb{E}\bigl[\,
     \mathbb{E}\bigl[\,W_t \mid W_{\,t-1}\bigr]\bigr]
  \;=\;
  \mathbb{E}\bigl[\,W_{\,t-1}\bigr] \;+\;\alpha,
$$

从而

$$
  \mathbb{E}\bigl[\,W_t\bigr] \;=\; 1 \;+\; t\,\alpha,\qquad t=0,1,2,\dots.
$$



同样地，条件在 $W_{\,t-1}=w$ 时，有两种情况：

* 以概率 $1 - \tfrac{1}{w}$，保持 $X_t=x$（即 $W_t=w$）。这时 $W_t^2 = w^2$。
* 以概率 $\tfrac{1}{w}$，令 $X_t=x+1$（即 $W_t = (1+\alpha)\,w$）。这时 $W_t^2 = (1+\alpha)^2 \,w^2$。

因此
$$
\begin{align*}
\mathbb{E}\bigl[W_t^2 \mid W_{t-1} = w\bigr]
&= \Bigl(1 - \tfrac{1}{w}\Bigr)w^2 +\tfrac{1}{w}\bigl((1+\alpha)^2 w^2 \bigr)\\
&= w^2 - w + (1+\alpha)^2w\\
&= w^2 + \bigl((1+\alpha)^2 - 1\bigr)w.
\end{align*}

$$
$$ 

设
$$
  \delta \;=\; (\,1+\alpha\,)^2 - 1 \;=\; 2\,\alpha \;+\;\alpha^2.
$$

那么上式可写成

$$
  \mathbb{E}\bigl[\,W_t^2 \mid W_{\,t-1}\bigr]
  \;=\; W_{\,t-1}^2 \;+\; \delta\,W_{\,t-1}.
$$

取全期望并用迭代展开：

$$
  \mathbb{E}\bigl[\,W_t^2\bigr]
  \;=\;
  \mathbb{E}\bigl[\,
    \mathbb{E}\bigl[\,W_t^2 \mid W_{\,t-1}\bigr]
  \bigr]
  \;=\;
  \mathbb{E}\bigl[\,W_{\,t-1}^2\bigr]
  \;+\;
  \delta\,\mathbb{E}\bigl[\,W_{\,t-1}\bigr].
$$

因为 $\mathbb{E}[W_{\,t-1}] = 1 + (t-1)\alpha$，可得到

$$
  \mathbb{E}\bigl[\,W_t^2\bigr]
  \;=\;
  \mathbb{E}\bigl[\,W_{\,t-1}^2\bigr]
  \;+\;
  \delta\bigl[\;1 + (t-1)\alpha\;\bigr].
$$

注意初始时 $W_0=1$，故 $\mathbb{E}[W_0^2]=1$。于是对 $t=1,2,\dots$ 逐步累加：

$$
  \mathbb{E}\bigl[\,W_t^2\bigr]
  \;=\; 1 \;+\;\sum_{i=1}^t \delta\bigl[\,1 + (i-1)\alpha\bigr]
  \;=\; 1 \;+\;\delta\,\sum_{i=0}^{\,t-1}\bigl[\,1 + i\,\alpha\,\bigr].
$$

计算一下求和：

$$
  \sum_{i=0}^{\,t-1} \bigl(1 + i\,\alpha\bigr)
  \;=\; \sum_{i=0}^{\,t-1} 1 \;+\;\alpha\,\sum_{i=0}^{\,t-1} i
  \;=\; t \;+\;\alpha\cdot\frac{(t-1)\,t}{2}.
$$

因此

$$
  \mathbb{E}\bigl[\,W_t^2\bigr]
  \;=\; 1 \;+\; \delta\,\Bigl[t + \alpha\cdot\frac{(t-1)\,t}{2}\Bigr].
$$

代回 $\delta = 2\alpha + \alpha^2$，可写成更展开的形式，但我们只要最后拿去与 $\bigl[\mathbb{E}(W_t)\bigr]^2$ 相减，就可得到方差。

先列出：

1. $\displaystyle \mathbb{E}\bigl[\,W_t\bigr] = 1 + t\,\alpha.$
2. $$
     \mathbb{E}\bigl[\,W_t^2\bigr]
     = 1 
     \;+\; (2\alpha + \alpha^2)\,t 
     \;+\; (2\alpha + \alpha^2)\,\alpha\,\frac{(t-1)\,t}{2}.
   $$

   将第二项与第三项的系数合并：

   * $(2\alpha + \alpha^2)\,t = 2\,\alpha\,t + \alpha^2\,t.$
   * $(2\alpha + \alpha^2)\,\alpha\,\frac{(t-1)\,t}{2}  = \frac{(t-1)\,t}{2}\,\bigl(2\alpha^2 + \alpha^3\bigr).$

合并之后：

$$
  \mathbb{E}\bigl[\,W_t^2\bigr]
  = 1 
  \;+\; \bigl(2\,\alpha\,t + \alpha^2\,t\bigr)
  \;+\; \bigl(2\,\alpha^2 + \alpha^3\bigr)\,\frac{(t-1)\,t}{2}.
$$

而

$$
  \bigl[\mathbb{E}(W_t)\bigr]^2 = \bigl(1 + t\,\alpha\bigr)^2
  = 1 \;+\; 2\,t\,\alpha \;+\; t^2\,\alpha^2.
$$

于是
$$
\begin{align*}
\mathrm{Var}\bigl[W_t\bigr]
&= \mathbb{E}\bigl[W_t^2\bigr] - \bigl[\mathbb{E}(W_t)\bigr]^2\\
&= \bigl[1 + 2\alpha t + \alpha^2 t + (2\alpha^2 + \alpha^3)\tfrac{(t-1),t}{2}\bigr]
-\bigl[1 + 2t\alpha + t^2\alpha^2\bigr]\\
&= \alpha^2t ;+; (2\alpha^2 + \alpha^3)\frac{(t-1)t}{2}
- t^2\alpha^2\\
&= \alpha^2t + \bigl(\alpha^22 + \alpha^3\bigr)\frac{(t^2 - t)}{2}
- \alpha^2t^2\\
&= \alpha^2t + \bigl(\alpha^2\tfrac{2}{2} + \alpha^3\tfrac{1}{2}\bigr)(t^2 - t)
- \alpha^2t^2\\
&= \alpha^2t
+ \bigl(\alpha^2 + \tfrac{\alpha^3}{2}\bigr)t^2
- \bigl(\alpha^2 + \tfrac{\alpha^3}{2}\bigr)t
- \alpha^2t^2\\
&= \alpha^2t
+ \alpha^2t^2 + \tfrac{\alpha^3}{2}t^2
- \alpha^2t - \tfrac{\alpha^3}{2}t
- \alpha^2t^2\\
&= \frac{\alpha^3}{2}\bigl(t^2 - t\bigr)
= \frac{\alpha^3}{2}t(t-1).
\end{align*}
$$



因此，对任意 $t$，
$$
  \mathbb{E}[\,W_t\,] \;=\; 1 + t\,\alpha,
  \qquad
  \mathrm{Var}[\,W_t\,] \;=\; \frac{\alpha^3}{2}\;t\,\bigl(t-1\bigr).
$$

由于题目中定义
$$
  \hat n \;=\; \frac{\,W_{\,n-1}\,}{\alpha},
$$

于是直接做线性变换即可：

1. **期望**

   $$
     \mathbb{E}[\hat n]
     \;=\; \mathbb{E}\Bigl[\;\tfrac{W_{\,n-1}}{\alpha}\Bigr]
     \;=\; \frac{1}{\alpha}\,\mathbb{E}[\,W_{\,n-1}\,]
     \;=\; \frac{1}{\alpha}\Bigl[\,1 + (n-1)\alpha\Bigr]
     \;=\; \frac{1}{\alpha} + (n-1)\;=\; n + \frac{1-\alpha}{\alpha}.
   $$

   有时也可以写成

   $$
     \mathbb{E}[\hat n]
     \;=\; \frac{1 + (n-1)\,\alpha}{\alpha}.
   $$
2. **方差**

   $$
     \mathrm{Var}[\hat n]
     \;=\; \mathrm{Var}\Bigl[\;\tfrac{W_{\,n-1}}{\alpha}\Bigr]
     \;=\; \frac{1}{\alpha^2}\,\mathrm{Var}[\,W_{\,n-1}\,]
     \;=\; \frac{1}{\alpha^2}\,\cdot\frac{\alpha^3}{2}\;(n-1)(n-2)
     \;=\; \frac{\alpha\,\bigl(n-1\bigr)\bigl(n-2\bigr)}{2}.
   $$

于是我们得到了完整答案：

> **结论**
>
> $$
>   \boxed{
>     \mathbb{E}[\hat n] 
>     \;=\; \frac{1 + (n-1)\,\alpha}{\alpha}
>     \;=\; n + \frac{1-\alpha}{\alpha}
>     ,
>     \qquad
>     \mathrm{Var}[\hat n]
>     \;=\; \frac{\alpha\,\bigl(n-1\bigr)\bigl(n-2\bigr)}{2}.
>   }
> $$

---

**2. 设计一个 $(1\pm\epsilon)$-相对误差、失败概率至多 $\delta$ 的算法，并分析空间开销**

我们已经知道单一份“$\alpha$-Morris 计数器”在做完 $n$ 次更新后输出的 $\hat n$，其期望不完全等于 $n$，而是 $\displaystyle n + \tfrac{1-\alpha}{\alpha}$，并且方差约为 $\tfrac{\alpha}{2}(n-1)(n-2)$。为了得到一个**近似相对误差**控制在 $\epsilon\,n$ 以内且失败概率不超过 $\delta$ 的估计，我们需要“多路并行运行若干份这样的计数器”并做适当聚合（例如Median‐of‐Means 或多数投票、取中值等方法）。下面我们给出一个标准的“多份独立计数 + 取中位数”策略，具体思路如下。

1. **运行 $L$ 份独立的“$\alpha$-Morris 计数器”**，每一份都有自己的随机过程和随机输出。将它们标号为 $C^{(1)},\,C^{(2)},\,\dots,\,C^{(L)}$。

   * 每一份计数器从头到尾都以 $\alpha$-Morris 的规则工作，累加 $n$ 次更新。第 $i$ 份最后输出为

     $$
       \hat n_i 
       \;=\; \frac{(1+\alpha)^{\,X^{(i)}_{\,n-1}}}{\alpha},
     $$

     其中 $X^{(i)}_{\,t}$ 表示“第 $i$ 份计数器在 $t$ 次更新后”的随机值。

2. **从这 $L$ 次输出 $\{\hat n_1,\dots,\hat n_L\}$ 中取中位数**（即将这 $L$ 个数从小到大排序，选第 $\lceil L/2\rceil$ 个或 $\lfloor L/2\rfloor$ 个作为最终估计 $\widetilde n$）。

   * 记最终输出为 $\widetilde n = \mathrm{median}\{\hat n_1,\dots,\hat n_L\}.$

3. **选择合适的 $L$（与 $\epsilon,\delta$ 有关）**，使得

   $$
     \Pr\Bigl[\,\bigl|\widetilde n - n \bigr| > \epsilon\,n\,\Bigr] \;\le\; \delta.
   $$

   由此就得到了一个以失败概率 $\le\delta$ 给出“相对误差 $\le\epsilon$”的近似计数算法：
   
   ```
   参数：ε, δ ∈ (0,1)。（先固定一个常数型 α，使 α ≤ ε²/3 且 α∈(0,1]。）
   
   1.   设 L ← 48 · ln(1/δ).    (向上取整为整数)
   2.   并行启动 L 个独立的 α‐Morris 计数器 Counter[i]（i = 1…L），每个计数器 
        内部仅存一个整数 X[i]←0，每次更新时以概率 (1+α)^{−X[i]} 让 X[i]++。
   
   3.   对流中的每一条“+1 更新”操作，依次让 i = 1…L 中所有的 Counter[i] 同步做一次“用概率 (1+α)^{-X[i]} 增加 X[i]”的单步更新。  （L 个是并行地做）
   
   4.   在做完总共 n 次更新后（此时每个 Counter[i] 内都有它自己的随机值 
          X[i] = X_i^{(n)}, 
        我们令它们对应的估计值 
          ĥ[i] ← (1+α)^{ X_i^{(n)−1} } / α.   // 根据“算法描述”中的定义，是用 X 的值在 n−1 步时的结果。
   
        （如果 n 很小，令 n < 2(1−α)/(αε)，我们退到“直接精确累加一个普通整数计数器”去，用 O(log n) 位直接输出 n 即可。）
   
   5.   在所有 L 个 ĥ[1…L] 中取“中位数”：
          ŷ ← Median( ĥ[1], ĥ[2], …, ĥ[L] ).
   
   6.   输出 ŷ 作为最终对 n 的估计。 
   ```
   

**正确性验证**

我们先固定考察**其中的一份** $\alpha$-Morris 计数器（下标省去），它在 $n$ 次更新后给出一个随机输出 $\hat n$。我们已经计算出

$$
  \mathbb{E}[\hat n] 
  \;=\; n + \tfrac{1-\alpha}{\alpha},
  \qquad
  \mathrm{Var}[\hat n] 
  \;=\; \frac{\alpha\,(n-1)(n-2)}{2}.
$$

为了要把它用来估计 $n$，我们希望 $\hat n$ “足够集中”在 $n$ 附近，换句话说，需要保证

$$
  \Pr\Bigl[\,\bigl|\hat n - n\bigr| > \epsilon\,n\,\Bigr]
  \;\le\; p,
$$

其中 $p$ 可以取一个小常数（例如 $p=\tfrac{1}{3}$）。之后再做“$L$ 份独立计数后取中位数”时，将总失败概率压到 $\delta$。

先用 Chebyshev （切比雪夫不等式）来对单个 $\hat n$ 做一个宽松的尾部界。

* **偏置估计**：

  $$
    \bigl|\mathbb{E}[\hat n] - n\bigr|
    \;=\;
    \Bigl|\,n + \tfrac{1-\alpha}{\alpha} - n\Bigr|
    \;=\;
    \frac{1-\alpha}{\alpha}.
  $$

  在 $\alpha$ 为常数的前提下，这个偏置也是一个 $O(1)$ 的常数，而我们关心的是相对误差 $ \epsilon\,n$。只要 $n$ 足够大（$\,n \gg \tfrac{1}{\alpha\epsilon}\,$），那么偏置 $\frac{1-\alpha}{\alpha}$ 相较于 $\epsilon\,n$ 可以忽略。为了方便起见，我们可以从此处开始 **假设 $n$ 已经“足够大”，使得**

  $$
    \frac{1-\alpha}{\alpha} \;\le\; \frac{\epsilon\,n}{2},
  $$

  也就是 $n \ge \tfrac{2(1-\alpha)}{\alpha\,\epsilon}$。（如果 $n$ 真的很小，则我们可以用“直接在内存里精确计数”来处理，空间开销 $\log n = O(\log (1/\epsilon))$ 也很小，不是重点。）

  在这种假设下，想要保证“$\hat n$ 与 $\mathbb{E}[\hat n]$ 相差不超过 $\tfrac{\epsilon\,n}{2}$”，就一定能保证“$\hat n$ 与 $n$ 的相对误差 $\le\epsilon$”。具体地：

  $$
    \bigl|\hat n - n\bigr|
    \le \bigl|\hat n - \mathbb{E}[\hat n]\bigr|
         + \bigl|\mathbb{E}[\hat n] - n\bigr|
    \;\le\; 
    \bigl|\hat n - \mathbb{E}[\hat n]\bigr|
    \;+\;\frac{\epsilon\,n}{2}.
  $$

  于是若我们只要保证

  $$
    \bigl|\hat n - \mathbb{E}[\hat n]\bigr|
    \;\le\; \frac{\epsilon\,n}{2},
  $$

  则必推出 $\bigl|\hat n - n\bigr|\le \epsilon\,n$。

* **应用切比雪夫不等式**：
  切比雪夫不等式告诉我们，对任何随机变量 $X$，

  $$
    \Pr\Bigl[\;|X - \mathbb{E}[X]|\;\ge\; t\Bigr]
    \;\le\;
    \frac{\mathrm{Var}[X]}{\,t^2\,}.
  $$

  令 $X = \hat n$，并取

  $$
    t \;=\; \frac{\epsilon\,n}{2}.
  $$

  那么

  $$
    \Pr\Bigl[\;|\hat n - \mathbb{E}[\hat n]|\;\ge\;\tfrac{\epsilon\,n}{2}\Bigr]
    \;\le\;
    \frac{\mathrm{Var}[\hat n]}{\bigl(\tfrac{\epsilon\,n}{2}\bigr)^2}
    \;=\;
    \frac{\;\tfrac{\alpha\,(n-1)(n-2)}{2}\;}{\;\tfrac{\epsilon^2\,n^2}{4}\;}
    \;=\;
    \frac{2\,\alpha\,(n-1)(n-2)}{\;\epsilon^2\,n^2\;}\;\cdot\;\frac{1}{2}
    \;=\;
    \frac{\alpha\,(n-1)(n-2)}{\;\epsilon^2\,n^2\;} .
  $$

  对于“大 $n$” 来说，上式大致算成

  $$
    \frac{\alpha\,n^2}{\epsilon^2\,n^2} 
    \;=\; \frac{\alpha}{\epsilon^2}.
  $$

  只要我们**再取 $\alpha$ 足够小**——但题目中已经说 $\alpha$ 是常数，倾向于取“任意固定的小常数”即可——那么

  $$
    \Pr\Bigl[\;|\hat n - \mathbb{E}[\hat n]|\ge \tfrac{\epsilon\,n}{2}\Bigr]
    \;=\; O\Bigl(\tfrac{\alpha}{\epsilon^2}\Bigr).
  $$

  只要保证 $\tfrac{\alpha}{\epsilon^2}\le \tfrac13$，即可让单次试验失败概率约为 $\tfrac13$。事实上，我们可以先固定 $\epsilon$，然后取“某个绝对常数”的 $\alpha$（例如 $\alpha = \min\{\,1/2,\;\epsilon^2/3\}$）满足

  $$
    \frac{\alpha}{\epsilon^2} \;\le\; \frac{1}{3}.
  $$

  这样便得到了：

  $$
    \Pr\Bigl[\;|\hat n - \mathbb{E}[\hat n]| \ge \tfrac{\epsilon\,n}{2}\Bigr]
    \;\le\; \frac13.
  $$

  结合前面的偏置论证，就得到单次“$\alpha$-Morris 计数器”输出 $\hat n$ 与 $n$ 的相对误差超出 $\epsilon$ 的概率至多 $\tfrac13$。

  **总结**：我们设定了固定常数 $\alpha$ 使得

  $$
    \Pr\bigl[\,|\hat n - n|\;>\;\epsilon\,n\,\bigr] \;\le\; \frac13,
  $$

  （略去 $n$ 较小时“直接精确计数”的情况，保证 $n\ge\frac{2(1-\alpha)}{\alpha\,\epsilon}$。）

既然我们已经让“单个计数器”在估计 “$\hat n$ 与 $n$” 的相对误差 $\le\epsilon$ 这一事件发生的成功概率达到至少 $2/3$，那我们只需把“失败概率”等比缩小到 $\delta$”即可。

1. **并行启动 $L$ 个独立副本**

   * 记这 $L$ 份计数器的输出分别为 $\hat n_1, \hat n_2,\dots,\hat n_L$。
   * 根据上一节的结论，对每一个 $i=1,2,\dots,L$，

     $$
       \Pr\Bigl[\;|\hat n_i - n| > \epsilon\,n\Bigr] \;\le\; \tfrac13,
       \quad
       \Pr\Bigl[\;|\hat n_i - n| \le \epsilon\,n\Bigr] \;\ge\; \tfrac23.
     $$
   * 并行运行 $L$ 份，所得的 $\hat n_i$ 之间相互独立。

2. **取中位数（median）**
   令 $\widetilde n = \mathrm{median}\bigl(\,\hat n_1,\dots,\hat n_L\bigr)$。我们要让

   $$
     \Pr\Bigl[\;|\widetilde n - n| > \epsilon\,n\Bigr] \;\le\; \delta.
   $$

   中位数的优点在于，只要“超过半数的 $\hat n_i$ 均在 $[\,n(1-\epsilon),\,n(1+\epsilon)\,]$ 区间内”，那么中位数 $\widetilde n$ 也一定会落在这个区间里。换言之，

   $$
     \bigl|\widetilde n - n\bigr| > \epsilon\,n
     \;\Longrightarrow\;
     \text{至少有 }L/2\text{ 个}~\hat n_i 
     \text{的误差是 }>\epsilon\,n.
   $$

   设

   $$
     Y_i \;=\; 
     \begin{cases}
       1, & \text{若 }|\hat n_i - n|\le \epsilon\,n,\\
       0, & \text{若 }|\hat n_i - n| > \epsilon\,n.
     \end{cases}
   $$

   则 $\{Y_i\}_{i=1}^L$ 是独立的 Bernoulli 随机变量，且

   $$
     \Pr[Y_i = 1] \;\ge\; \tfrac23,\qquad
     \Pr[Y_i = 0] \;\le\; \tfrac13.
   $$

   我们需要保证“少于 $L/2$ 个 $Y_i$ 取 1”的概率至多为 $\delta$。等价地，我们要控制

   $$
     \Pr\Bigl[\; \text{在这 }L\text{ 个试验中，}Y_i=1\text{ 的次数 } 
       \;<\; \frac{L}{2} \Bigr]
     \;\le\; \delta.
   $$

   这正是从 $L$ 次独立 Bernoulli$(p=\tfrac23)$ 试验中，求“1 的次数少于 $L/2$”的尾部事件。由**Chernoff 边界**，取 $p=2/3$、目标阈值 $L/2$，可写成：

   $$
     \Pr\Bigl[\,\#\{i: Y_i=1\} < L/2\Bigr]
     \;=\; 
     \Pr\Bigl[\,\#\{i: Y_i=1\} < \bigl(1 - \eta\bigr)\,\tfrac{2L}{3}\Bigr]
   $$

   其中我们需要满足 $(1-\eta)(2/3) L = L/2$。解得

   $$
     \frac{2}{3}\,(1 - \eta)\;=\;\frac12
     \;\Longrightarrow\;
     1 - \eta \;=\; \frac34
     \;\Longrightarrow\;
     \eta \;=\; \frac14.
   $$

   于是用 Chernoff 不等式（一种常见形式是：若 $Z = \sum_{i=1}^L Z_i$ 且每个 $Z_i$ 都是独立的 Bernoulli$(p)$，那么

   $$
     \Pr\bigl[\,Z < (1-\eta)\,p\,L\bigr]
     \;\le\; \exp\bigl(\,-\,\tfrac{\eta^2\,p\,L}{2}\bigr)
     \quad\text{对于 }0<\eta<1
   \]),
   我们有
   \[
     \Pr\bigl[\,\#\{Y_i=1\} < L/2\,\bigr]
     \;=\; \Pr\Bigl[\,\#\{Y_i=1\} < (1 - \tfrac14)\tfrac{2L}{3}\Bigr]
     \;\le\;
     \exp\Bigl(\,-\,\tfrac{(\tfrac14)^2 \cdot \tfrac{2}{3}\,L}{2}\Bigr)
     \;=\;
     \exp\Bigl(\,-\,\tfrac{\,2\,L\,}{\,3\cdot 2 \cdot 16\,}\Bigr)
     \;=\;
     \exp\Bigl(\,-\,\tfrac{L}{\,48\,}\Bigr).
   $$

   因此，只要我们取

   $$
     \exp\Bigl(\!-\,\tfrac{L}{48}\Bigr) \;\le\; \delta
     \;\Longleftrightarrow\;
     L \;\ge\; 48\,\ln\!\bigl(\tfrac{1}{\delta}\bigr),
   $$

   那么就能保证

   $$
     \Pr\bigl[\,|\widetilde n - n| > \epsilon\,n\,\bigr]
     \;\le\; \delta.
   $$

   也就是说，只要我们并行“独立地”运行 $L=O\bigl(\ln(1/\delta)\bigr)$ 份 $\alpha$-Morris 计数器，然后把它们的输出取中位数，就可把失败概率压到 $\delta$。

**空间复杂度分析**

综合至今，我们需要并行维护 $L$ 份独立的 Morris 计数器。下面分别估算它们的比特空间开销。

1. **单份 $\alpha$-Morris 计数器的空间**

   * 在任意时刻，该计数器只要存储一个整数 $X \ge 0$ 即可。
   * 由于 “$X$ 的增长速率” 大约要到 $\Theta\bigl(\log_{\,1+\alpha}(n)\bigr) = \Theta\bigl(\tfrac{\ln n}{\ln(1+\alpha)}\bigr)$ 左右才有可能抵达接近“使 $(1+\alpha)^X\approx n$” 的规模。因此 $X$ 的最大可能大小上界为 $O(\log_{\,1+\alpha} n)$，对应的比特数就是

     $$
       O\Bigl(\;\log\bigl(\log_{\,1+\alpha}n\bigr)\Bigr)
       \;=\;
       O\bigl(\log \ln n \;-\; \log \ln(1+\alpha)\bigr)
       \;=\; 
       O\bigl(\log\ln n\bigr),
     $$

     （因为 $\alpha$ 是常数，则 $\ln(1+\alpha)$ 也是常数）。
   * 所以，每份计数器大概需要 $O(\log\log n)$ 位来存放它的当前 $X$。

2. **并行 $L$ 份计数器的总空间**

   * 我们需要 $L$ 份完全独立的随机化流程、每份都保存一个整数 $X^{(i)}$。因此空间一共是

     $$
       L \times O\bigl(\log\log n\bigr) 
       \;=\; O\bigl(\ln(1/\delta)\bigr)\times O\bigl(\log\log n\bigr)
       \;=\; O\bigl(\log(1/\delta)\,\cdot\,\log\log n\bigr).
     $$

3. **为了保证总体失败概率 $\le\delta$，我们取 $L = 48 \ln(1/\delta)$。**

   * 这样，就最终得到：

     $$
       \text{总空间} 
       \;=\; L \cdot O(\log\log n)
       \;=\; O\bigl(\ln\tfrac{1}{\delta}\bigr)\;\times\;O(\log\log n)
       \;=\; O\bigl(\log\tfrac{1}{\delta}\,\cdot\,\log\log n\bigr).
     $$
   * 由于题目允许“最坏空间复杂度”只要是 $\mathrm{poly}\bigl(\tfrac{1}{\delta},\,\tfrac{1}{\delta'},\,\tfrac{1}{\epsilon},\,\log\log n\bigr)$ 级别，而我们现在仅用了 $\epsilon$ 在选择 $\alpha$ 时是一常数（不再显式出现在 L 中）， 所以总体可以写成

     $$
       O\bigl(\ln\tfrac{1}{\delta}\bigr)\cdot O(\log\log n)
       \;=\; \operatorname{poly}\!\Bigl(\tfrac{1}{\delta},\,\log\log n\Bigr).
     $$
   * 若题目还需要对失败概率 $\delta'$ 附加另一层保证（例如某些场景里还要再保证“算法总体偶尔重启的成功率 $\ge1-\delta'$”），那再添一层 $\ln(1/\delta')$ 因子，也仍然是在同样那种“多项式”范围之内。

---

> Problem 思考题．
> 1．我们介绍了 rectified flow，知道 rectified flow 试图求动态最优传输的概率路径，从而使得扩散过程的推理步数降低。那么，有没有其他的做法，来让扩散方程的梯度项尽量和时间无关？
>
> 2．大语言模型（如 LLaMA、GPT）通常需要在大规模语料上进行微调（例如 instruction tuning 或 domain adaptation）。为了降低计算成本，研究者尝试用 Coreset 方法从上千万条训练数据中选出一个具有代表性的子集（如 $5 \%$ ）进行训练。如果你无法访问模型梯度信息（例如使用 GPT－4 API），你如何设计一个 ＂无监督＂的 Coreset 策略？你会如何定义＂代表性＂？

**1. 学习“时间分离”的 Score Parameterization**

**核心目标**：尽量把 $s_\theta(x,t)$ 写成 $\widetilde s_\theta(x)$ 和有关 $t$ 的一两个简单函数的乘积/加和，从而在训练时减少对每个 $t$ 的独立参数。

假设在一定范围内，$\nabla_x\log p_t(x)\;\approx\; \alpha(t)\cdot q_\theta(x)\;+\;\beta(t)\cdot r_\theta(x)\,,$

  其中 $\alpha(t),\beta(t)$ 只与时间有关，$q_\theta(x)$ 和 $r_\theta(x)$ 是两个与 $x$ 相关的固定网络（没有直接取决于 $t$）。

* 优点：训练时只需要去学两个“纯空间网络” $q_\theta,r_\theta$，以及一对时间缩放系数 $\{\alpha(t),\beta(t)\}$。

* 约束：必须在相当宽的 $t$ 范围内保证上述“线性分离”近似成立。也就是说，所有噪声水平下的 score 均近似落在 $\mathrm{span}\{q_\theta(x),\,r_\theta(x)\}$ 之内。训练中可以直接最小化


$$
\mathbb{E}_{t,x}\Bigl[\bigl\|\,s_\theta(x,t)\;-\;\bigl(\alpha(t)\,q_\theta(x)+\beta(t)\,r_\theta(x)\bigr)\bigr\|^2\Bigr].

$$
如果实际场景里需要更多项，就把分解写成


$$
s_\theta(x,t)\;\approx\;\sum_{i=1}^K w_i(t)\,u_i(x),
$$
 先固定一组“空间基函数” $\{u_i(x)\}_{i=1}^K$，再只学 $K$ 个纯空间网络，以及 $K$ 个时间函数 $\{w_i(t)\}$。如果 $K$ 很小，就可以大幅削弱网络对“连续时间依赖”的需求。

**2.  **

1. **只用 API 打分／嵌入**：我们假设可以对每条文本（prompt/response）调用 GPT‐4 或 LLaMA‐Embedding API，获得：

* **Embedding 向量**：一个高维稠密向量 $e_i \in \mathbb{R}^d$，可视为“GPT‐4 对这条文本的语义表示”。

* **Perplexity/似然分数**：用 GPT‐4 给原始大数据跑一个语言模型打分，得到每条文本的“困惑度”或“对数似然”。

2. **代表性的定义**：常见的思考指标包括：



* **覆盖度（Coverage）**：在“Embedding 空间”里，所选的子集尽可能覆盖大多数样本的分布。可以用 k‐Center、K‐Medoids、Greedy‐cover 等方法。

* **分布失真最小化（Distributional Fidelity）**：让子集在 Embedding 上的一阶/二阶矩与原始全集匹配。常见做法是最小化子集与全集在 PCA 空间的重构误差。

* **样本稀疏度**：优先保留那些在原始语料里**更具代表性**、但不算“太冷门”或“噪声”的示例；可用 GPT‐4 的 perplexity 来衡量“这条文本与大语料的贴近度”，理想选择“perplexity 在中间段”（既不频繁出现，也不太“绝无仅有”）。

* **互信息 / 多样性**：如果既要保证子集“覆盖绝大多数主题”，又要“去除很多高度相似或重复”的文本，可以用 DPP（Determinantal Point Process）或其它多样性采样方法，让子集中的 Embedding 互相之间差异最大化。



​	在实际设计时，只要能把“代表性”写成一个可度量的函数，就能沿着这个思路做贪心选点或随机/加权采样。

下面给出一种算法思想：

#### 2.1 步骤一：用大模型获取文本表示和初步权重



1. **对每条训练样本 $D = \{x_1,\dots,x_N\}$ 调用 Embedding API**：



- 得到 $\{e_1,e_2,\dots,e_N\}$，其中 $e_i\in\mathbb{R}^d$ 是高维向量。

- 同时用 LM API 计算每个 $x_i$ 的 Perplexity（或 PPL），称为 $\mathrm{ppl}(x_i)$。

2. **对 Embedding 做维度降维**：



* 如果 $N$ 极大，或者 Embedding 维度 $d$ 也很高，可以先用 PCA 或随机投影把所有 $e_i$ 投影到低维空间 $\mathbb{R}^r$，例如 $r=50$ 或 $r=100$。

* 这样后续在 $\mathbb{R}^r$ 空间里做聚类、距离计算、覆盖度测试都更快。

3. **为每条样本定义一个“初步得分”**：



   常见做法是 $\omega_i = 1$（默认不加权）；如果想优先保重“perplexity 中间”或去掉“过于平凡/过于离群”的样本，可以令


$$
\omega_i 

​       \;=\; 

​       \exp\Bigl(-\gamma \,\bigl|\log \mathrm{ppl}(x_i) - \mu_{\log \mathrm{ppl}}\bigr|\Bigr),
$$


​     其中 $\mu_{\log \mathrm{ppl}}$ 是整批数据的平均 $\log$-PPL，$\gamma$ 是一个缩放因子。这样会淡化“过低 PPL”（流水线式、毫无信息）或者“过高 PPL”（极端离群、难学）的示例，而偏向“中间态”的、既不平庸也不极端的样本。

如果只是想最大化覆盖度，也可以令 $\omega_i\equiv 1$，全部视为同等。

到此为止，每条样本 $x_i$ 都拥有：

* 一串低维表示 $\widetilde e_i \in \mathbb{R}^r$。

* 一个非负权重 $\omega_i$。



#### 2.2 步骤二：构造“层次化”的数据分段



当 $N$ 特别大时，直接在上千万点里做一次 O($N^2$) 的 Greedy‐k‐Center 会太慢。我们可以先做分层粗聚类来降低复杂度，同时保留结构信息。



1. **第一层聚类（Coarse‐Level）**



* 用快速的 “Mini‐Batch k‐Means” 或“Online k‐Means”在 $\{\widetilde e_i\}$ 上预先做 $M$ 个聚类中心（典型地 $M\approx 1000$ 或 $5000$，视数据规模而定）。

* 将全集划分为 $\mathcal{C}_1,\dots,\mathcal{C}_M$ 这 $M$ 个簇。每个簇 $\mathcal{C}_j$ 内的样本在 Embedding 空间里比较接近。

2. **第二层聚类（Fine‐Level within each Coarse cluster）**



* 对每个大簇 $\mathcal{C}_j$（假设其大小为 $N_j$），再用“同样的 Greedy‐Center” 或“小样本 k‐Center” 做二次细分。

* 这样就能保证 “同一个大簇内的样本距离相对有限”，细分后就可以在一个更小的范围里挑代表点。

#### 2.3 步骤三：加权 Greedy‐k‐Center / Coreset 选点



在每个“最细分的小簇”里，我们就是在一个相对**低维、规模更小**、**同质化**更高的小集合里选几个点。具体做法：



1. **在第 $j$ 个大簇 $\mathcal{C}_j$ 内部**，我们有一组样本下标集合 $\{i: x_i\in\mathcal{C}_j\}$。



2. **针对这一小簇使用**“加权 Greedy‐Center”：

  先令 “当前已选代表集” $S_j=\varnothing$。

  重复下列操作直至达到所需样本数比例（例如想把总体 5% 分配到每个大簇上时，就按簇内总大小的 5% 做限制）：



​     1. 在簇内所有尚未被选作代表的点中，计算其与当前 $S_j$ 中所有已选点的最小距离（Euclidean in $\mathbb{R}^r$ 或者余弦距离）。

​     2. 令下一个要加入 $S_j$ 的样本是“使 $\omega_i \times (\min_{s\in S_j}\|\,\widetilde e_i - \widetilde e_s\|)$” 最大的那个 $i$。也就是说，这时要把“距离当前代表集最远，同时权重大”的点挑出来。这样可以保证：

- 选出的点在 Embedding 空间里“互相离得远”，对簇内分布有很强的覆盖；

- 如果我们给某些“perplexity 中段”或“语料稀有但有代表性”的样本更大的 $\omega_i$，就更容易保留那些asp采样。

- 直到达到“簇内大小 $\times \alpha$ ” 的上限。比如大簇 $\mathcal{C}_j$ 有 $N_j$ 条文本，用“局部覆盖度”策略选出大约 $\lceil \alpha\,N_j\rceil$ 条。



3. **合并所有簇的代表点**



* 最后把 $\bigcup_{j=1}^M S_j$ 作为全局 coreset。这样总体大小近似 $\sum_j \alpha\,N_j = \alpha\,N$。比如 $\alpha=0.05$ 就代表 5% 的子集。

* 每个子集 $S_j$ 内部保证权重加权后一致的覆盖度，而不同大簇之间，因为本身做了第一层粗聚类，代表点也自然覆盖了多种“语义主题”。

