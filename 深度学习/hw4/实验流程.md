### 深度学习作业4实现流程详解

以下是针对CIFAR-10自监督学习实验的详细实现步骤，包含代码级关键细节说明：

---

### 一、数据准备
1. **数据集加载**
```python
import torchvision
from torchvision import transforms

# 加载10%子集（需自定义采样逻辑）
def load_subset(train=True, ratio=0.1):
    full_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=train, download=True
    )
    subset_size = int(len(full_dataset) * ratio)
    indices = torch.randperm(len(full_dataset))[:subset_size]
    return torch.utils.data.Subset(full_dataset, indices)
```

2. **数据增强实现**
```python
def get_augmentations(normalize=True):
    color_jitter = transforms.ColorJitter(
        brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2
    )
    transform_list = [
        transforms.RandomResizedCrop(32, scale=(0.08, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomApply([color_jitter], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor()
    ]
    if normalize:
        transform_list.append(transforms.Normalize(
            mean=[0.4914, 0.4822, 0.4465],
            std=[0.2023, 0.1994, 0.2010]
        ))
    return transforms.Compose(transform_list)

# 关键点：对每个样本应用两次不同的增强
class TwoCropTransform:
    def __init__(self, transform):
        self.transform = transform

    def __call__(self, x):
        return [self.transform(x), self.transform(x)]
```

---

### 二、模型构建
1. **Base Encoder (ResNet18)**
```python
import torch.nn as nn
from torchvision.models import resnet18

class BaseEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = resnet18(pretrained=False)
        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.model.maxpool = nn.Identity()  # 取消原ResNet的下采样
        self.model.fc = nn.Identity()  # 去掉分类层
        
    def forward(self, x):
        return self.model(x)
```

2. **Projection Head**
```python
class ProjectionHead(nn.Module):
    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
    def forward(self, x):
        return self.mlp(x)
```

---

### 三、对比损失实现
```python
import torch
import torch.nn.functional as F

def nt_xent_loss(z, temperature=0.5):
    device = z.device
    batch_size = z.shape[0]
    
    # 计算相似度矩阵
    z_norm = F.normalize(z, dim=1)
    sim_matrix = torch.mm(z_norm, z_norm.T) / temperature
    
    # 排除对角线元素
    mask = ~torch.eye(2*batch_size, dtype=torch.bool, device=device)
    sim_matrix = sim_matrix[mask].reshape(2*batch_size, -1)
    
    # 构造正样本对标签（每对i, j的索引）
    labels = torch.arange(batch_size, device=device)
    labels = torch.cat([labels + batch_size - 1, labels])
    
    # 计算交叉熵损失
    loss = F.cross_entropy(sim_matrix, labels)
    return loss
```

---

### 四、自监督训练流程
```python
from torch.utils.data import DataLoader

# 初始化
encoder = BaseEncoder().to(device)
projection = ProjectionHead().to(device)
optimizer = torch.optim.Adam(
    list(encoder.parameters()) + list(projection.parameters()),
    lr=1e-3, weight_decay=1e-4
)

# 数据加载
train_dataset = load_subset(train=True, ratio=0.1)
aug_dataset = TwoCropTransform(get_augmentations())
loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

# 训练循环
for epoch in range(100):
    for imgs, _ in loader:
        x1, x2 = imgs  # shape: [B, 3, 32, 32]
        x = torch.cat([x1, x2], dim=0).to(device)
        
        # 前向传播
        h = encoder(x)
        z = projection(h)
        
        # 计算损失
        loss = nt_xent_loss(z)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

### 五、线性评估协议
```python
class LinearClassifier(nn.Module):
    def __init__(self, encoder, num_classes=10):
        super().__init__()
        self.encoder = encoder
        for param in self.encoder.parameters():
            param.requires_grad = False  # 冻结编码器
        self.fc = nn.Linear(512, num_classes)
        
    def forward(self, x):
        h = self.encoder(x)
        return self.fc(h)

# 微调训练
clf = LinearClassifier(encoder).to(device)
criterion = nn.CrossEntropyLoss()
opt = torch.optim.Adam(clf.parameters(), lr=3e-4)

# 使用完整训练集（非子集）
full_train_loader = DataLoader(
    torchvision.datasets.CIFAR10(..., train=True),
    batch_size=512, shuffle=True
)

for epoch in range(30):
    for x, y in full_train_loader:
        x, y = x.to(device), y.to(device)
        logits = clf(x)
        loss = criterion(logits, y)
        # ...反向传播步骤同上
```

---

### 六、实验分析建议
1. **数据增强对比实验**
```python
# 创建不同增强组合的变体
aug_v1 = get_augmentations_without_color_jitter()
aug_v2 = get_augmentations_with_strong_jitter()
# 记录不同组合下的分类准确率
```

2. **温度系数影响分析**
```python
temps = [0.05, 0.1, 0.5, 1.0]
for temp in temps:
    # 使用不同温度参数训练模型
    # 对比最终分类准确率
```

3. **可视化中间表示**
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def visualize_tsne(encoder, dataloader):
    features, labels = [], []
    for x, y in dataloader:
        features.append(encoder(x).detach().cpu())
        labels.append(y)
    # TSNE降维并绘制散点图
```

---

### 关键实现注意事项
1. **显存优化**：使用混合精度训练 (`torch.cuda.amp`) 和梯度累积可缓解大batch size需求

2. **负样本采样**：当batch size较小时，可结合MoCo的memory bank机制

3. **学习率调度**：使用余弦退火等策略改善训练稳定性

4. **模型保存**：定期保存编码器参数用于后续分析

5. **硬件适配**：通过`torch.utils.data.DataLoader`的num_workers参数优化数据加载速度

完整实现代码约需300-500行，建议使用PyTorch Lightning框架简化训练循环。